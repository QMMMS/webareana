{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855e583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def judge_node_same(obs_nodes_info_a, obs_nodes_info_b, threshold=3):\n",
    "    \"\"\"\n",
    "    判断两个状态之间的节点差异是否超过阈值\n",
    "    obs_nodes_info_a 和 b 都是一个字典，字典的key是节点id，value 是一个字典，例如：\n",
    "    '11765': {'backend_id': 19269,\n",
    "    'union_bound': [0.0, 0.0, 10.0, 10.0],\n",
    "    'text': \"[11765] RootWebArea 'Postmill' focused: True\"}\n",
    "\n",
    "    对于 obs_nodes_info_a 的一个元素，如果 obs_nodes_info_b 中存在一个元素，其 union_bound 一致，并且 text 去掉 [xxx] 后一致，则认为两个元素是相同的\n",
    "    如果不同的元素小于 threshold，则认为两个状态是相同的\n",
    "    这里所谓的 key 和 backend_id 都不重要，不作为参考依据\n",
    "\n",
    "    返回值：\n",
    "    - 如果两个状态差异小于等于 threshold，相同，返回 True\n",
    "    - 如果两个状态差异超过阈值，返回 False\n",
    "    \"\"\"\n",
    "\n",
    "    diff_count = 0\n",
    "\n",
    "    b_nodes_set = set()\n",
    "    for node_id, node_info in obs_nodes_info_b.items():\n",
    "        union_bound = node_info['union_bound']\n",
    "        cleaned_text = re.sub(r'\\[.*?\\]', '', node_info['text'])\n",
    "        b_nodes_set.add(f\"{union_bound}_{cleaned_text}\")\n",
    "\n",
    "    for node_id, node_info in obs_nodes_info_a.items():\n",
    "        union_bound = node_info['union_bound']\n",
    "        cleaned_text = re.sub(r'\\[.*?\\]', '', node_info['text'])\n",
    "        if f\"{union_bound}_{cleaned_text}\" not in b_nodes_set:\n",
    "            diff_count += 1\n",
    "    # print(diff_count)\n",
    "    return diff_count <= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402eb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a11y_to_components(a11y):\n",
    "    \"\"\"\n",
    "    将 a11y 转换为组件列表\n",
    "    \"\"\"\n",
    "    # 对于每一行\n",
    "    # 如果包含 [xxx]，则去掉 [xxx]，将后面的内容视作一个组件\n",
    "\n",
    "    components = []\n",
    "    for line in a11y.split(\"\\n\"):\n",
    "        # 去掉前后空格\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        match = re.match(r'\\[(\\d+)\\]', line)\n",
    "        if match:\n",
    "            # component_id = match.group(1)\n",
    "            component_content = line[match.end():].strip()\n",
    "            components.append(component_content)\n",
    "        else:\n",
    "            components.append(line)\n",
    "    return components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74a27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_node_same_from_a11y(a11y_a, a11y_b, threshold=3):\n",
    "    \"\"\"\n",
    "    判断两个状态之间的节点差异是否超过阈值\n",
    "    a11y_a 和 b 都是一个字符串，字符串的格式为例如：\n",
    "    [5207] heading 'relationship_advice — relationship_advice'\n",
    "        [5209] link 'relationship_advice — relationship_advice'\n",
    "    [5215] button 'Subscribe No subscribers'\n",
    "        [5571] generic 'No subscribers'\n",
    "    [5216] StaticText '5,721 submissions'\n",
    "\n",
    "    首先去掉前面的 [xxx]，将后面的内容视作一个组件\n",
    "\n",
    "    对于 a11y_a 的一个组件，如果 a11y_b 中存在一个组件，其组件内容一致，则认为两个组件是相同的\n",
    "    如果不同的组件小于 threshold，则认为两个状态是相同的\n",
    "\n",
    "    返回值：\n",
    "    - 如果两个状态差异小于等于 threshold，相同，返回 True\n",
    "    - 如果两个状态差异超过阈值，返回 False\n",
    "    \"\"\"\n",
    "\n",
    "    diff_count = 0\n",
    "\n",
    "    a_components = a11y_to_components(a11y_a)\n",
    "    b_components = a11y_to_components(a11y_b)\n",
    "\n",
    "    for a_component in a_components:\n",
    "        if a_component not in b_components:\n",
    "            diff_count += 1\n",
    "    # print(diff_count)\n",
    "    return diff_count <= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b8d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "def bm25_retrieval(query, corpus, top_n=3):\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    tokenized_query = query.split(\" \")\n",
    "    top_n_docs = bm25.get_top_n(tokenized_query, corpus, n=top_n)\n",
    "    top_n_docs_index = [corpus.index(doc) for doc in top_n_docs]\n",
    "    return top_n_docs_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c53abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# # /home/zjusst/qms/webarena/result_stage_1_explore/trajs 底下的所有文件名\n",
    "# file_names = os.listdir(\"/home/zjusst/qms/webarena/result_stage_1_explore/trajs\")\n",
    "# file_names = [file_name for file_name in file_names if file_name.endswith(\".json\")]\n",
    "# # file_names.sort() 自定义排序，按照 _ 分割，取后面的数字排序\n",
    "# file_names.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "# len(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:32<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# unique_page_ids = []  # (file_name, traj_idx, 0|1代表前还是后, real_url) \n",
    "# unique_page_a11ys = []\n",
    "# #unique_page_nodes = []\n",
    "# triplets = []  # (page_id, action, page_id)\n",
    "\n",
    "# for file_name in tqdm(file_names):\n",
    "#     with open(os.path.join(\"/home/zjusst/qms/webarena/result_stage_1_explore/trajs\", file_name), \"r\") as f:\n",
    "#         trajs = json.load(f)\n",
    "\n",
    "#     for traj_idx, traj in enumerate(trajs):\n",
    "#         if len(unique_page_ids) == 0:\n",
    "#             unique_page_ids.append((file_name, traj_idx, 0, traj['url_real_before']))\n",
    "#             unique_page_a11ys.append(traj[\"a11y_before\"])\n",
    "#             #unique_page_nodes.append(traj[\"state_before\"]['text']['obs_nodes_info'])\n",
    "\n",
    "#         prev_page_id = -1\n",
    "#         # 从 unique_page_a11ys 检索出最接近的三个\n",
    "#         top_n_docs_index = bm25_retrieval(traj[\"a11y_before\"], unique_page_a11ys, 3)\n",
    "#         for doc_idx in top_n_docs_index:\n",
    "#             # if judge_node_same(traj[\"state_before\"]['text']['obs_nodes_info'], unique_page_nodes[doc_idx], 3):\n",
    "#             #     prev_page_id = unique_page_ids[doc_idx]\n",
    "#             #     break\n",
    "#             if judge_node_same_from_a11y(traj[\"a11y_before\"], unique_page_a11ys[doc_idx], 3):\n",
    "#                 prev_page_id = unique_page_ids[doc_idx]\n",
    "#                 break\n",
    "#         if prev_page_id == -1:\n",
    "#             unique_page_ids.append((file_name, traj_idx, 0, traj['url_real_before']))\n",
    "#             unique_page_a11ys.append(traj[\"a11y_before\"])\n",
    "#             #unique_page_nodes.append(traj[\"state_before\"]['text']['obs_nodes_info'])\n",
    "#             prev_page_id = unique_page_ids[-1]\n",
    "\n",
    "#         action_str = traj[\"action_str\"]\n",
    "\n",
    "#         after_page_id = -1\n",
    "#         top_n_docs_index = bm25_retrieval(traj[\"a11y_after\"], unique_page_a11ys, 3)\n",
    "#         for doc_idx in top_n_docs_index:\n",
    "#             # if judge_node_same(traj[\"state_after\"]['text']['obs_nodes_info'], unique_page_nodes[doc_idx], 3):\n",
    "#             #     after_page_id = unique_page_ids[doc_idx]\n",
    "#             #     break\n",
    "#             if judge_node_same_from_a11y(traj[\"a11y_after\"], unique_page_a11ys[doc_idx], 3):\n",
    "#                 after_page_id = unique_page_ids[doc_idx]\n",
    "#                 break\n",
    "#         if after_page_id == -1:\n",
    "#             unique_page_ids.append((file_name, traj_idx, 1, traj['url_real_after']))\n",
    "#             unique_page_a11ys.append(traj[\"a11y_after\"])\n",
    "#             #unique_page_nodes.append(traj[\"state_after\"]['text']['obs_nodes_info'])\n",
    "#             after_page_id = unique_page_ids[-1]\n",
    "            \n",
    "#         triplets.append((prev_page_id, action_str, after_page_id))\n",
    "\n",
    "# print(len(triplets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# with open(\"/home/zjusst/qms/webarena/result_stage_1_explore/flitered_triplets.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"prev_page_id\", \"action_str\", \"after_page_id\"])\n",
    "#     writer.writerows(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75404efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 /home/zjusst/qms/webarena/result_stage_1_explore/flitered_triplets.csv\n",
    "with open(\"/home/zjusst/qms/webarena/result_stage_1_explore/flitered_triplets.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    # 跳过第一行\n",
    "    next(reader)\n",
    "    triplets = [row for row in reader]\n",
    "len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f10b2fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('812_1.json', 1, 1, 'http://reddit.com/wiki')\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc418166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"('812_2.json', 8, 1, 'http://reddit.com/forums')\", 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_degree_dict = {}\n",
    "for triplet in triplets:\n",
    "    if triplet[2] not in out_degree_dict:\n",
    "        out_degree_dict[triplet[2]] = 0\n",
    "    out_degree_dict[triplet[2]] += 1\n",
    "\n",
    "# 转为 list，由低到高排序\n",
    "out_degree_list = sorted(out_degree_dict.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "034b51e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flitered_out_degree_list = []\n",
    "for item in out_degree_list:\n",
    "    real_url = eval(item[0])[3]\n",
    "    # 如果 real_url 的  \"/\" 大于 3 个，则加入 flitered_out_degree_list\n",
    "    if real_url.count(\"/\") > 3 and real_url.startswith(\"http://reddit.com/\"):\n",
    "        flitered_out_degree_list.append(item)\n",
    "\n",
    "len(flitered_out_degree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def random_select_url_and_a11y(flitered_out_degree_list):\n",
    "    # 种子为时间\n",
    "    random.seed(time.time())\n",
    "    record = flitered_out_degree_list[random.randint(0, len(flitered_out_degree_list) - 1)]\n",
    "    file_name, traj_index, pos, real_url = eval(record[0])\n",
    "    with open(f\"/home/zjusst/qms/webarena/result_stage_1_explore/trajs/{file_name}\", \"r\") as f:\n",
    "        trajs = json.load(f)\n",
    "    traj = trajs[traj_index]\n",
    "    if pos == 0:\n",
    "        a11y = traj[\"a11y_before\"]\n",
    "    else:\n",
    "        a11y = traj[\"a11y_after\"]\n",
    "    return real_url, a11y\n",
    "# random_select_url_and_a11y(flitered_out_degree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68bf9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = []\n",
    "with open(\"/home/zjusst/qms/persona-hub/data/elite_personas_10000.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        persona = json.loads(line)\n",
    "        personas.append(persona['persona'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf00fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A geologist who is passionate about glacial studies and has a deep understanding of the geology of Antarctica. They are knowledgeable about the history and current state of glacial movements in the area, and have extensive experience in mapping and analyzing glacial ice sheets. They are also skilled in using satellite imagery and other geospatial data to study glacial processes and their impact on the environment. Additionally, they have a keen interest in the history and culture of the people who have lived and worked in Antarctica, and have a deep appreciation for the natural beauty and challenges of the region.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def random_persona(personas):\n",
    "    random.seed(time.time())\n",
    "    return personas[random.randint(0, len(personas) - 1)]\n",
    "\n",
    "random_persona(personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://reddit.com/featured/hot',\n",
       " \"Tab 0 (current): Postmill\\n\\n[9738] RootWebArea 'Postmill' focused: True\\n\\t[9768] HeaderAsNonLandmark '\")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_url, a11y = random_select_url_and_a11y(flitered_out_degree_list)\n",
    "real_url, a11y[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webarena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
